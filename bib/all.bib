Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{pythia8dot1,
abstract = {The Pythia program is a standard tool for the generation of high-energy collisions, comprising a coherent set of physics models for the evolution from a few-body hard process to a complex multihadronic final state. It contains a library of hard processes and models for initial- and final-state parton showers, multiple parton-parton interactions, beam remnants, string fragmentation and particle decays. It also has a set of utilities and interfaces to external programs. While previous versions were written in Fortran, Pythia 8 represents a complete rewrite in C++. The current release is the first main one after this transition, and does not yet in every respect replace the old code. It does contain some new physics aspects, on the other hand, that should make it an attractive option especially for LHC physics studies. Program summary: Program title: Pythia 8.1. Catalogue identifier: ACTU_v3_0. Program summary URL: http://cpc.cs.qub.ac.uk/summaries/ACTU_v3_0.html. Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland. Licensing provisions: GPL version 2. No. of lines in distributed program, including test data, etc.: 176 981. No. of bytes in distributed program, including test data, etc.: 2 411 876. Distribution format: tar.gz. Programming language: C++. Computer: Commodity PCs. Operating system: Linux; should also work on other systems. RAM: 8 megabytes. Classification: 11.2. Does the new version supersede the previous version?: yes, partly. Nature of problem: High-energy collisions between elementary particles normally give rise to complex final states, with large multiplicities of hadrons, leptons, photons and neutrinos. The relation between these final states and the underlying physics description is not a simple one, for two main reasons. Firstly, we do not even in principle have a complete understanding of the physics. Secondly, any analytical approach is made intractable by the large multiplicities. Solution method: Complete events are generated by Monte Carlo methods. The complexity is mastered by a subdivision of the full problem into a set of simpler separate tasks. All main aspects of the events are simulated, such as hard-process selection, initial- and final-state radiation, beam remnants, fragmentation, decays, and so on. Therefore events should be directly comparable with experimentally observable ones. The programs can be used to extract physics from comparisons with existing data, or to study physics at future experiments. Reasons for new version: Improved and expanded physics models, transition from Fortran to C++. Summary of revisions: New user interface, transverse-momentum-ordered showers, interleaving with multiple interactions, and much more. Restrictions: Depends on the problem studied. Running time: 10-1000 events per second, depending on process studied. References: [1] T. Sj{\"{o}}strand, P. Ed{\'{e}}n, C. Friberg, L. L{\"{o}}nnblad, G. Miu, S. Mrenna, E. Norrbin, Comput. Phys. Comm. 135 (2001) 238. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {0710.3820},
author = {Sj{\"{o}}strand, Torbj{\"{o}}rn and Mrenna, Stephen and Skands, Peter},
doi = {10.1016/j.cpc.2008.01.036},
eprint = {0710.3820},
issn = {00104655},
journal = {Comput. Phys. Commun.},
keywords = {Event generators,Hadronisation,Monte Carlo,Multiparticle production,Multiple interactions,Parton showers},
mendeley-tags = {Monte Carlo},
month = {jun},
number = {11},
pages = {852--867},
publisher = {North-Holland},
title = {{A brief introduction to PYTHIA 8.1}},
volume = {178},
year = {2008}
}
@article{A3tune,
abstract = {A tune of the PPPPPP 8 event generator suitable for inclusive QCD modelling is presented. The A3 tune uses the early Run 2 charged particle distribution and inelastic cross section results from ATLAS in addition to the Run 1 data used previously to construct minimum-bias tunes. For the first time in ATLAS, the tuning considers diffraction modelling parameters and a diffractive model other than the PPPPPP 8 default is used. This results in a better description of the measured inelastic cross-sections and a level of agreement that is comparable to the previous A2 tune for the other distributions considered. This can lead to improved modelling of additional proton-proton collisions in simulation.},
author = {{The ATLAS Collaboration}},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The ATLAS Collaboration - 2016 - The Pythia 8 A3 tune description of ATLAS minimum bias and inelastic measurements incorporating the Don.pdf:pdf},
journal = {ATL-PHYS-PUB-2016-017},
keywords = {MCGENERATORS,Minimum Bias,Pythia8,Tune},
month = {aug},
title = {{The Pythia8 A3 tune description of ATLAS minimum bias and inelastic measurements incorporating the Donnachie-Landshoff diffractive model}},
url = {https://cds.cern.ch/record/2206965 https://inspirehep.net/record/1477266},
year = {2016}
}
@techreport{ATLAS-TDR-TDAQ-PhaseII,
address = {Geneva},
author = {{The ATLAS Collaboration}},
booktitle = {ATLAS-TDR-029},
file = {:home/harry/Documents/phd/l1calo/resources/tdaq-phase2-tdr_2017.pdf:pdf},
institution = {CERN},
title = {{Technical Design Report for the Phase-II Upgrade of the ATLAS TDAQ System}},
url = {https://cds.cern.ch/record/2285584},
year = {2017}
}
@article{Hayes1989,
abstract = {The bootstrap statistical method is applied to the discrepancy in the one-charged-particle decay modes of the tau lepton. This eliminates questions about the correctness of the errors ascribed to the branching-fraction measurements and the use of Gaussian error distributions for systematic errors. The discrepancy is still seen when the results of the bootstrap analysis are combined with other measurements and with deductions from theory. But the bootstrap method assigns less statistical significance to the discrepancy compared to a method using Gaussian error distributions. {\textcopyright} 1989 The American Physical Society.},
author = {Hayes, Kenneth G. and Perl, Martin L. and Efron, Bradley},
doi = {10.1103/PhysRevD.39.274},
issn = {05562821},
journal = {Phys. Rev. D},
month = {jan},
number = {1},
pages = {274--279},
publisher = {American Physical Society},
title = {{Application of the bootstrap statistical method to the tau-decay-mode problem}},
url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.39.274},
volume = {39},
year = {1989}
}
@inproceedings{Deviveiros2019,
author = {Deviveiros, Pier-Olivier},
month = {aug},
publisher = {ATLAS Internal},
title = {{Run 3 L1 algorithm and trigger performance forum}},
url = {https://indico.cern.ch/event/839580/},
year = {2019}
}
@article{AZNLOtune,
abstract = {This paper describes a measurement of the Z/$\gamma$* boson transverse momentum spectrum using ATLAS proton-proton collision data at a centre-of-mass energy of (Formula presented) TeV at the LHC. The measurement is performed in the Z/$\gamma$* → e+e− and Z/$\gamma$* → $\mu$+$\mu$− channels, using data corresponding to an integrated luminosity of 4.7 fb−1. Normalized differential cross sections as a function of the Z/$\gamma$* boson transverse momentum are measured for transverse momenta up to 800 GeV. The measurement is performed inclusively for Z/$\gamma$* rapidities up to 2.4, as well as in three rapidity bins. The channel results are combined, compared to perturbative and resummed QCD calculations and used to constrain the parton shower parameters of Monte Carlo generators.},
archivePrefix = {arXiv},
arxivId = {1406.3660},
author = {{The ATLAS Collaboration}},
doi = {10.1007/JHEP09(2014)145/METRICS},
eprint = {1406.3660},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aad et al. - 2014 - Measurement of the Z$\gamma$ boson transverse momentum distribution in pp collisions at √s = 7 TeV with the ATLAS detect.pdf:pdf},
issn = {10298479},
journal = {J. High Energy Phys.},
keywords = {Hadron-Hadron Scattering},
month = {sep},
number = {9},
pages = {1--47},
publisher = {Springer Verlag},
title = {{Measurement of the Z/$\gamma*$ boson transverse momentum distribution in pp collisions at $\sqrt{s}$ = 7 TeV with the ATLAS detector}},
url = {https://link.springer.com/article/10.1007/JHEP09(2014)145},
volume = {2014},
year = {2014}
}
@article{ATLAS2020a,
abstract = {The ATLAS detector at the Large Hadron Collider reads out particle collision data from over 100 million electronic channels at a rate of approximately 100 kHz, with a recording rate for physics events of approximately 1 kHz. Before being certified for physics analysis at computer centres worldwide, the data must be scrutinised to ensure they are clean from any hardware or software related issues that may compromise their integrity. Prompt identification of these issues permits fast action to investigate, correct and potentially prevent future such problems that could render the data unusable. This is achieved through the monitoring of detector-level quantities and reconstructed collision event characteristics at key stages of the data processing chain. This paper presents the monitoring and assessment procedures in place at ATLAS during 2015-2018 data-taking. Through the continuous improvement of operational procedures, ATLAS achieved a high data quality efficiency, with 95.6\% of the recorded proton-proton collision data collected at s=13 TeV certified for physics analysis.},
archivePrefix = {arXiv},
arxivId = {physics.ins-det/1911.04632},
author = {{The ATLAS Collaboration}},
doi = {10.1088/1748-0221/15/04/P04003},
eprint = {1911.04632},
issn = {17480221},
journal = {J. Instrum.},
keywords = {Large detector systems for particle and astroparti,Large detector-systems performance},
number = {4},
pages = {P04003},
primaryClass = {physics.ins-det},
title = {{ATLAS data quality operations and performance for 2015-2018 data-taking}},
volume = {15},
year = {2020}
}
@article{NNPDF3dot0,
abstract = {We present NNPDF3.0, the first set of parton distribution functions (PDFs) determined with a methodology validated by a closure test. NNPDF3.0 uses a global dataset including HERA-II deep-inelastic inclusive cross-sections, the combined HERA charm data, jet production from ATLAS and CMS, vector boson rapidity and transverse momentum distributions from ATLAS, CMS and LHCb, W +c data from CMS and top quark pair production total cross sections from ATLAS and CMS. Results are based on LO, NLO and NNLO QCD theory and also include electroweak corrections. To validate our methodology, we show that PDFs determined from pseudo-data generated from a known underlying law correctly reproduce the statistical distributions expected on the basis of the assumed experimental uncertainties. This closure test ensures that our methodological uncertainties are negligible in comparison to the generic theoretical and experimental uncertainties of PDF determination. This enables us to determine with confidence PDFs at different perturbative orders and using a variety of experimental datasets ranging from HERA-only up to a global set including the latest LHC results, all using precisely the same validated methodology. We explore some of the phenomenological implications of our results for the upcoming 13 TeV Run of the LHC, in particular for Higgs production cross-sections.},
archivePrefix = {arXiv},
arxivId = {1410.8849},
author = {Ball, Richard D. and Bertone, Valerio and Carrazza, Stefano and Deans, Christopher S. and {Del Debbio}, Luigi and Forte, Stefano and Guffanti, Alberto and Hartland, Nathan P. and Latorre, Jos{\'{e}} I. and Rojo, Juan and Ubiali, Maria},
doi = {10.1007/JHEP04(2015)040},
eprint = {1410.8849},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ball et al. - 2015 - Parton distributions for the LHC run II.pdf:pdf},
issn = {1029-8479},
journal = {J. High Energy Phys. 2015 20154},
keywords = {Classical and Quantum Gravitation,Elementary Particles,Quantum Field Theories,Quantum Field Theory,Quantum Physics,Relativity Theory,String Theory},
month = {apr},
number = {4},
pages = {1--148},
publisher = {Springer},
title = {{Parton distributions for the LHC run II}},
url = {https://link.springer.com/article/10.1007/JHEP04(2015)040},
volume = {2015},
year = {2015}
}
@article{ct10,
abstract = {We extract new parton distribution functions (PDFs) of the proton by global analysis of hard scattering data in the general-mass framework of perturbative quantum chromodynamics. Our analysis includes new theoretical developments together with the most recent collider data from deep-inelastic scattering, vector boson production, and single-inclusive jet production. Because of the difficulty in fitting both the D0 Run-II W lepton asymmetry data and some fixed-target DIS data, we present two families of PDFs, CT10 and CT10W, without and with these high-luminosity W lepton asymmetry data included in the global analysis. With both sets of PDFs, we study theoretical predictions and uncertainties for a diverse selection of processes at the Fermilab Tevatron and the CERN Large Hadron Collider. {\textcopyright} 2010 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {1007.2241},
author = {Lai, Hung Liang and Guzzi, Marco and Huston, Joey and Li, Zhao and Nadolsky, Pavel M. and Pumplin, Jon and Yuan, C. P.},
doi = {10.1103/PhysRevD.82.074024},
eprint = {1007.2241},
issn = {15507998},
journal = {Phys. Rev. D - Part. Fields, Gravit. Cosmol.},
month = {oct},
number = {7},
pages = {074024},
publisher = {American Physical Society},
title = {{New parton distributions for collider physics}},
url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.82.074024},
volume = {82},
year = {2010}
}
@article{Efron1988,
abstract = {This is a survey of modern developments in statistical regression, written for the mathematically educated nonstatistician. It begins with a review of the traditional theory of least-squares curve-fitting. Modern developments in regression theory have developed in response to the practical limitations of the least-squares approach. Recent progress has been made feasible by the electronic computer, which frees statisticians from the confines of mathematical tractability. Topics discussed include robust regression, bootstrap measures of variability, local smoothing and cross-validation, projection pursuit, Mallows' Cp criterion, Stein estimation, generalized regression for Poisson data, and regression methods for censored data. All of the methods are illustrated with real-life examples.},
author = {Efron, Bradley},
doi = {10.1137/1030093},
issn = {00361445},
journal = {SIAM Rev.},
keywords = {62-02,62505,Poisson regression,Stein estimation,bootstrap,cross-validation Mallows' Cp,least absolute deviations,projection pursuit,robust regression},
month = {jul},
number = {3},
pages = {421--449},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Computer-intensive methods in statistical regression}},
url = {https://epubs.siam.org/doi/10.1137/1030093},
volume = {30},
year = {1988}
}
@article{madgraph5amc,
abstract = {We discuss the theoretical bases that underpin the automation of the computations of tree-level and next-to-leading order cross sections, of their matching to parton shower simulations, and of the merging of matched samples that differ by light-parton multiplicities. We present a computer program, MadGraph5 aMC@NLO, capable of handling all these computations — parton-level fixed order, shower-matched, merged — in a unified framework whose defining features are flexibility, high level of parallelisation, and human intervention limited to input physics quantities. We demonstrate the potential of the program by presenting selected phenomenological applications relevant to the LHC and to a 1-TeV e + e − collider. While next-to-leading order results are restricted to QCD corrections to SM processes in the first public version, we show that from the user viewpoint no changes have to be expected in the case of corrections due to any given renormalisable Lagrangian, and that the implementation of these are well under way.},
archivePrefix = {arXiv},
arxivId = {1405.0301},
author = {Alwall, J. and Frederix, R. and Frixione, S. and Hirschi, V. and Maltoni, F. and Mattelaer, O. and Shao, H. S. and Stelzer, T. and Torrielli, P. and Zaro, M.},
doi = {10.1007/JHEP07(2014)079},
eprint = {1405.0301},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alwall et al. - 2014 - The automated computation of tree-level and next-to-leading order differential cross sections, and their match(2).pdf:pdf},
issn = {1029-8479},
journal = {J. High Energy Phys. 2014 20147},
keywords = {Classical and Quantum Gravitation,Elementary Particles,Quantum Field Theories,Quantum Field Theory,Quantum Physics,Relativity Theory,String Theory},
month = {jul},
number = {7},
pages = {1--157},
publisher = {Springer},
title = {{The automated computation of tree-level and next-to-leading order differential cross sections, and their matching to parton shower simulations}},
url = {https://link.springer.com/article/10.1007/JHEP07(2014)079},
volume = {2014},
year = {2014}
}
@article{Sherpa2dot2,
abstract = {SHERPA is a general-purpose Monte Carlo event generator for the simulation of particle collisions in high-energy collider experiments. We summarise essential features and improvements of the SHERPA 2.2 release series, which is heavily used for event generation in the analysis and interpretation of LHC Run 1 and Run 2 data. We highlight a decade of developments towards ever higher precision in the simulation of particle-collision events.},
archivePrefix = {arXiv},
arxivId = {1905.09127},
author = {Bothmann, Enrico and Chahal, Gurpreet Singh and H{\"{o}}che, Stefan and Krause, Johannes and Krauss, Frank and Kuttimalai, Silvan and Liebschner, Sebastian and Napoletano, Davide and Sch{\"{o}}nherr, Marek and Schulz, Holger and Schumann, Steffen and Siegert, Frank},
doi = {10.21468/SCIPOSTPHYS.7.3.034/PDF},
eprint = {1905.09127},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bothmann et al. - 2019 - Event generation with Sherpa 2.2.pdf:pdf},
issn = {25424653},
journal = {SciPost Phys.},
month = {sep},
number = {3},
pages = {034},
publisher = {SciPost Foundation},
title = {{Event generation with Sherpa 2.2}},
volume = {7},
year = {2019}
}
@article{geant4,
abstract = {GEANT4 is a toolkit for simulating the passage of particles through matter. It includes a complete range of functionality including tracking, geometry, physics models and hits. The physics processes offered cover a comprehensive range, including electromagnetic, hadronic and optical processes, a large set of long-lived particles, materials and elements, over a wide energy range starting, in some cases, from 250 eV and extending in others to the TeV energy range. It has been designed and constructed to expose the physics models utilised, to handle complex geometries, and to enable its easy adaptation for optimal use in different sets of applications. The toolkit is the result of a worldwide collaboration of physicists and software engineers. It has been created exploiting software engineering and object-oriented technology and implemented in the C++ programming language. It has been used in applications in particle physics, nuclear physics, accelerator design, space engineering and medical physics. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Agostinelli, S. and Allison, J. and Amako, K. and Apostolakis, J. and Araujo, H. and Arce, P. and Asai, M. and Axen, D. and Banerjee, S. and Barrand, G. and Behner, F. and Bellagamba, L. and Boudreau, J. and Broglia, L. and Brunengo, A. and Burkhardt, H. and Chauvie, S. and Chuma, J. and Chytracek, R. and Cooperman, G. and Cosmo, G. and Degtyarenko, P. and Dell'Acqua, A. and Depaola, G. and Dietrich, D. and Enami, R. and Feliciello, A. and Ferguson, C. and Fesefeldt, H. and Folger, G. and Foppiano, F. and Forti, A. and Garelli, S. and Giani, S. and Giannitrapani, R. and Gibin, D. and {Gomez Cadenas}, J. J. and Gonzalez, I. and {Gracia Abril}, G. and Greeniaus, G. and Greiner, W. and Grichine, V. and Grossheim, A. and Guatelli, S. and Gumplinger, P. and Hamatsu, R. and Hashimoto, K. and Hasui, H. and Heikkinen, A. and Howard, A. and Ivanchenko, V. and Johnson, A. and Jones, F. W. and Kallenbach, J. and Kanaya, N. and Kawabata, M. and Kawabata, Y. and Kawaguti, M. and Kelner, S. and Kent, P. and Kimura, A. and Kodama, T. and Kokoulin, R. and Kossov, M. and Kurashige, H. and Lamanna, E. and Lampen, T. and Lara, V. and Lefebure, V. and Lei, F. and Liendl, M. and Lockman, W. and Longo, F. and Magni, S. and Maire, M. and Medernach, E. and Minamimoto, K. and {Mora de Freitas}, P. and Morita, Y. and Murakami, K. and Nagamatu, M. and Nartallo, R. and Nieminen, P. and Nishimura, T. and Ohtsubo, K. and Okamura, M. and O'Neale, S. and Oohata, Y. and Paech, K. and Perl, J. and Pfeiffer, A. and Pia, M. G. and Ranjard, F. and Rybin, A. and Sadilov, S. and di Salvo, E. and Santin, G. and Sasaki, T. and Savvas, N. and Sawada, Y. and Scherer, S. and Sei, S. and Sirotenko, V. and Smith, D. and Starkov, N. and Stoecker, H. and Sulkimo, J. and Takahata, M. and Tanaka, S. and Tcherniaev, E. and {Safai Tehrani}, E. and Tropeano, M. and Truscott, P. and Uno, H. and Urban, L. and Urban, P. and Verderi, M. and Walkden, A. and Wander, W. and Weber, H. and Wellisch, J. P. and Wenaus, T. and Williams, D. C. and Wright, D. and Yamada, T. and Yoshida, H. and Zschiesche, D.},
doi = {10.1016/S0168-9002(03)01368-8},
issn = {01689002},
journal = {Nucl. Instruments Methods Phys. Res. Sect. A Accel. Spectrometers, Detect. Assoc. Equip.},
keywords = {Distributed software development,Geometrical modelling,Object-oriented technology,Particle interactions,Simulation,Software engineering},
month = {jul},
number = {3},
pages = {250--303},
publisher = {North-Holland},
title = {{GEANT4 - A simulation toolkit}},
volume = {506},
year = {2003}
}
@article{pythia8dot2,
abstract = {The Pythia program is a standard tool for the generation of events in high-energy collisions, comprising a coherent set of physics models for the evolution from a few-body hard process to a complex multiparticle final state. It contains a library of hard processes, models for initial-and final-state parton showers, matching and merging methods between hard processes and parton showers, multiparton interactions, beam remnants, string fragmentation and particle decays. It also has a set of utilities and several interfaces to external programs. Pythia 8.2 is the second main release after the complete rewrite from Fortran to C++, and now has reached such a maturity that it offers a complete replacement for most applications, notably for LHC physics studies. The many new features should allow an improved description of data.},
archivePrefix = {arXiv},
arxivId = {1410.3012},
author = {Sj{\"{o}}strand, Torbj{\"{o}}rn and Ask, Stefan and Christiansen, Jesper R. and Corke, Richard and Desai, Nishita and Ilten, Philip and Mrenna, Stephen and Prestel, Stefan and Rasmussen, Christine O. and Skands, Peter Z.},
doi = {10.1016/J.CPC.2015.01.024},
eprint = {1410.3012},
issn = {0010-4655},
journal = {Comput. Phys. Commun.},
keywords = {Event generators,Hadronisation,Matching and merging,Matrix elements,Multiparticle production,Multiparton interactions,Parton showers},
month = {jun},
number = {1},
pages = {159--177},
publisher = {North-Holland},
title = {{An introduction to PYTHIA 8.2}},
volume = {191},
year = {2015}
}
@article{NNPDF3dot1,
abstract = {We present a new set of parton distributions, NNPDF3.1, which updates NNPDF3.0, the first global set of PDFs determined using a methodology validated by a closure test. The update is motivated by recent progress in methodology and available data, and involves both. On the methodological side, we now parametrize and determine the charm PDF alongside the light-quark and gluon ones, thereby increasing from seven to eight the number of independent PDFs. On the data side, we now include the D0 electron and muon W asymmetries from the final Tevatron dataset, the complete LHCb measurements of W and Z production in the forward region at 7 and 8 TeV, and new ATLAS and CMS measurements of inclusive jet and electroweak boson production. We also include for the first time top-quark pair differential distributions and the transverse momentum of the Z bosons from ATLAS and CMS. We investigate the impact of parametrizing charm and provide evidence that the accuracy and stability of the PDFs are thereby improved. We study the impact of the new data by producing a variety of determinations based on reduced datasets. We find that both improvements have a significant impact on the PDFs, with some substantial reductions in uncertainties, but with the new PDFs generally in agreement with the previous set at the one-sigma level. The most significant changes are seen in the light-quark flavor separation, and in increased precision in the determination of the gluon. We explore the implications of NNPDF3.1 for LHC phenomenology at Run II, compare with recent LHC measurements at 13 TeV, provide updated predictions for Higgs production cross-sections and discuss the strangeness and charm content of the proton in light of our improved dataset and methodology. The NNPDF3.1 PDFs are delivered for the first time both as Hessian sets, and as optimized Monte Carlo sets with a compressed number of replicas.},
author = {Ball, Richard D. and Bertone, Valerio and Carrazza, Stefano and Debbio, Luigi Del and Forte, Stefano and Groth-Merrild, Patrick and Guffanti, Alberto and Hartland, Nathan P. and Kassabov, Zahari and Latorre, Jos{\'{e}} I. and Nocera, Emanuele R. and Rojo, Juan and Rottoli, Luca and Slade, Emma and Ubiali, Maria},
doi = {10.1140/EPJC/S10052-017-5199-5},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ball et al. - 2017 - Parton distributions from high-precision collider data.pdf:pdf},
issn = {1434-6052},
journal = {Eur. Phys. J. C 2017 7710},
keywords = {Astronomy,Astrophysics and Cosmology,Elementary Particles,Hadrons,Heavy Ions,Measurement Science and Instrumentation,Nuclear Energy,Nuclear Physics,Quantum Field Theories,Quantum Field Theory,String Theory},
month = {oct},
number = {10},
pages = {1--75},
publisher = {Springer},
title = {{Parton distributions from high-precision collider data}},
url = {https://link.springer.com/article/10.1140/epjc/s10052-017-5199-5},
volume = {77},
year = {2017}
}
@article{Aad2017a,
abstract = {The reconstruction of the signal from hadrons and jets emerging from the proton–proton collisions at the Large Hadron Collider (LHC) and entering the ATLAS calorimeters is based on a three-dimensional topological clustering of individual calorimeter cell signals. The cluster formation follows cell signal-significance patterns generated by electromagnetic and hadronic showers. In this, the clustering algorithm implicitly performs a topological noise suppression by removing cells with insignificant signals which are not in close proximity to cells with significant signals. The resulting topological cell clusters have shape and location information, which is exploited to apply a local energy calibration and corrections depending on the nature of the cluster. Topological cell clustering is established as a well-performing calorimeter signal definition for jet and missing transverse momentum reconstruction in ATLAS.},
author = {{The ATLAS Collaboration}},
doi = {10.1140/epjc/s10052-017-5004-5},
file = {:home/harry/Documents/phd/l1calo/resources/1603.02934.pdf:pdf},
issn = {1434-6052},
journal = {Eur. Phys. J. C},
number = {7},
pages = {490},
title = {{Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1}},
url = {https://doi.org/10.1140/epjc/s10052-017-5004-5},
volume = {77},
year = {2017}
}
@article{powhegbox,
abstract = {In this work we illustrate the POWHEG BOX, a general computer code framework for implementing NLO calculations in shower Monte Carlo programs according to the POWHEG method. Aim of this work is to provide an illustration of the needed theoretical ingredients, a view of how the code is organized and a description of what a user should provide in order to use it.},
archivePrefix = {arXiv},
arxivId = {1002.2581},
author = {Alioli, Simone and Nason, Paolo and Oleari, Carlo and Re, Emanuele},
doi = {10.1007/JHEP06(2010)043},
eprint = {1002.2581},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alioli et al. - 2010 - A general framework for implementing NLO calculations in shower Monte Carlo programs The POWHEG BOX(2).pdf:pdf},
issn = {10298479},
journal = {J. High Energy Phys.},
keywords = {Hadronic colliders,NLO computations,QCD},
month = {jun},
number = {6},
pages = {1--58},
publisher = {Springer Verlag},
title = {{A general framework for implementing NLO calculations in shower Monte Carlo programs: The POWHEG BOX}},
url = {https://link.springer.com/article/10.1007/JHEP06(2010)043},
volume = {2010},
year = {2010}
}
@article{Aad2017b,
abstract = {The reconstruction of the signal from hadrons and jets emerging from the proton–proton collisions at the Large Hadron Collider (LHC) and entering the ATLAS calorimeters is based on a three-dimensional topological clustering of individual calorimeter cell signals. The cluster formation follows cell signal-significance patterns generated by electromagnetic and hadronic showers. In this, the clustering algorithm implicitly performs a topological noise suppression by removing cells with insignificant signals which are not in close proximity to cells with significant signals. The resulting topological cell clusters have shape and location information, which is exploited to apply a local energy calibration and corrections depending on the nature of the cluster. Topological cell clustering is established as a well-performing calorimeter signal definition for jet and missing transverse momentum reconstruction in ATLAS.},
author = {{The ATLAS Collaboration}},
doi = {10.1140/epjc/s10052-017-5004-5},
file = {:home/harry/Documents/phd/l1calo/resources/1603.02934.pdf:pdf},
issn = {1434-6052},
journal = {Eur. Phys. J. C},
number = {7},
pages = {490},
title = {{Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1}},
url = {https://doi.org/10.1140/epjc/s10052-017-5004-5},
volume = {77},
year = {2017}
}
@article{Efron1987,
abstract = {We consider the problem of setting approximate confidence intervals for a single parameter $\theta$ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, $\theta$̂ ± $\sigma$̂z($\alpha$), can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.},
author = {Efron, Bradley},
doi = {10.2307/2289144},
issn = {01621459},
journal = {J. Am. Stat. Assoc.},
month = {mar},
number = {397},
pages = {171},
publisher = {JSTOR},
title = {{Better Bootstrap Confidence Intervals}},
volume = {82},
year = {1987}
}
@article{Nason2004,
abstract = {I show that with simple extensions of the shower algorithms in Monte Carlo programs, one can implement NLO corrections to the hardest emission that overcome the problems of negative weighted events found in previous implementations. Simple variants of the same method can be used for an improved treatment of matrix element corrections in Shower Monte Carlo programs. {\textcopyright} SISSA/ISAS 2004.},
archivePrefix = {arXiv},
arxivId = {hep-ph/0409146},
author = {Nason, Paolo},
doi = {10.1088/1126-6708/2004/11/040},
eprint = {0409146},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huss et al. - 2004 - A new method for combining NLO QCD with shower Monte Carlo algorithms.pdf:pdf},
issn = {10298479},
journal = {J. High Energy Phys.},
keywords = {Hadronic Colliders,NLO Computations,Parton Model,QCD},
month = {dec},
number = {11},
pages = {1097--1124},
primaryClass = {hep-ph},
publisher = {IOP Publishing},
title = {{A new method for combining NLO QCD with shower Monte Carlo algorithms}},
url = {https://iopscience.iop.org/article/10.1088/1126-6708/2004/11/040 https://iopscience.iop.org/article/10.1088/1126-6708/2004/11/040/meta},
volume = {8},
year = {2004}
}
@inproceedings{Brawn2019,
note = {[ATLAS Internal]},
author = {Brawn, Ian},
file = {:home/harry/Documents/phd/l1calo/resources/L1Calo_191022_Overview.pdf:pdf},
month = {oct},
title = {{L1Calo Overview, Status, Installation \& Commissioning}},
url = {https://indico.cern.ch/event/829769/contributions/3572289},
year = {2019}
}
@article{VBSZy-CONF,
abstract = {test},
archivePrefix = {arXiv},
arxivId = {2305.19142},
author = {{The ATLAS Collaboration}},
eprint = {2305.19142},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The ATLAS Collaboration - 2021 - Measurement of the cross-section of the electroweak production of a $Z gamma$ pair in association with.pdf:pdf},
keywords = {ATLAS Notes,CERN Document Server,WebSearch},
month = {jul},
title = {{Measurement of the cross-sections of the electroweak and total production of a $Z \gamma$ pair in association with two jets in $pp$ collisions at $\sqrt{s}$ = 13 TeV with the ATLAS detector}},
url = {http://cds.cern.ch/record/2779171 http://arxiv.org/abs/2305.19142},
year = {2023}
}
@article{ATLASsim1,
abstract = {The simulation software for the ATLAS Experiment at the Large Hadron Collider is being used for large-scale production of events on the LHC Computing Grid. This simulation requires many components, from the generators that simulate particle collisions, through packages simulating the response of the various detectors and triggers. All of these components come together under the ATLAS simulation infrastructure. In this paper, that infrastructure is discussed, including that supporting the detector description, interfacing the event generation, and combining the GEANT4 simulation of the response of the individual detectors. Also described are the tools allowing the software validation, performance testing, and the validation of the simulated output against known physics processes. {\textcopyright} 2010 CERN for the benefit of the ATLAS collaboration.},
archivePrefix = {arXiv},
arxivId = {1005.4568},
author = {{The ATLAS Collaboration}},
doi = {10.1140/epjc/s10052-010-1429-9},
eprint = {1005.4568},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/The ATLAS Collaboration - 2010 - The ATLAS Simulation Infrastructure.pdf:pdf},
isbn = {1005201014299},
issn = {14346052},
journal = {Eur. Phys. J. C},
keywords = {Astronomy,Astrophysics and Cosmology,Elementary Particles,Hadrons,Heavy Ions,Measurement Science and Instrumentation,Nuclear Energy,Nuclear Physics,Quantum Field Theories,Quantum Field Theory,String Theory},
month = {sep},
number = {3},
pages = {823--874},
publisher = {Springer},
title = {{The ATLAS Simulation Infrastructure}},
url = {https://link.springer.com/article/10.1140/epjc/s10052-010-1429-9},
volume = {70},
year = {2010}
}
@article{NNPDF2dot3,
abstract = {We present the first determination of parton distributions of the nucleon at NLO and NNLO based on a global data set which includes LHC data: NNPDF2.3. Our data set includes, besides the deep inelastic, Drell-Yan, gauge boson production and jet data already used in previous global PDF determinations, all the relevant LHC data for which experimental systematic uncertainties are currently available: ATLAS and LHCb W and Z rapidity distributions from the 2010 run, CMS W electron asymmetry data from the 2011 run, and ATLAS inclusive jet cross-sections from the 2010 run. We introduce an improved implementation of the FastKernel method which allows us to fit to this extended data set, and also to adopt a more effective minimization methodology. We present the NNPDF2.3 PDF sets, and compare them to the NNPDF2.1 sets to assess the impact of the LHC data. We find that all the LHC data are broadly consistent with each other and with all the older data sets included in the fit. We present predictions for various standard candle cross-sections, and compare them to those obtained previously using NNPDF2.1, and specifically discuss the impact of ATLAS electroweak data on the determination of the strangeness fraction of the proton. We also present collider PDF sets, constructed using only data from HERA, the Tevatron and the LHC, but find that this data set is neither precise nor complete enough for a competitive PDF determination. {\textcopyright} 2012 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1207.1303},
author = {Ball, Richard D. and Bertone, Valerio and Carrazza, Stefano and Deans, Christopher S. and {Del Debbio}, Luigi and Forte, Stefano and Guffanti, Alberto and Hartland, Nathan P. and Latorre, Jos{\'{e}} I. and Rojo, Juan and Ubiali, Maria},
doi = {10.1016/j.nuclphysb.2012.10.003},
eprint = {1207.1303},
issn = {05503213},
journal = {Nucl. Phys. B},
keywords = {Collider physics,LHC,Parton distributions,QCD},
month = {feb},
number = {2},
pages = {244--289},
publisher = {North-Holland},
title = {{Parton distributions with LHC data}},
volume = {867},
year = {2013}
}
@article{Aaboud2017a,
abstract = {This paper describes the implementation and performance of a particle flow algorithm applied to 20.2 fb$$^{-1}$$of ATLAS data from 8 TeV proton–proton collisions in Run 1 of the LHC. The algorithm removes calorimeter energy deposits due to charged hadrons from consideration during jet reconstruction, instead using measurements of their momenta from the inner tracker. This improves the accuracy of the charged-hadron measurement, while retaining the calorimeter measurements of neutral-particle energies. The paper places emphasis on how this is achieved, while minimising double-counting of charged-hadron signals between the inner tracker and calorimeter. The performance of particle flow jets, formed from the ensemble of signals from the calorimeter and the inner tracker, is compared to that of jets reconstructed from calorimeter energy deposits alone, demonstrating improvements in resolution and pile-up stability.},
author = {{The ATLAS Collaboration}},
doi = {10.1140/epjc/s10052-017-5031-2},
issn = {1434-6052},
journal = {Eur. Phys. J. C},
number = {7},
pages = {466},
title = {{Jet reconstruction and performance using particle flow with the ATLAS Detector}},
url = {https://doi.org/10.1140/epjc/s10052-017-5031-2},
volume = {77},
year = {2017}
}
@article{powheg,
abstract = {The aim of this work is to describe in detail the POWHEG method, first suggested by one of the authors, for interfacing parton-shower generators with NLO QCD computations. We describe the method in its full generality, and then specify its features in two subtraction frameworks for NLO calculations: the Catani-Seymour and the Frixione-Kunszt-Signer approach. Two examples are discussed in detail in both approaches: the production of hadrons in e +e - collisions, and the Drell-Yan vector-boson production in hadronic collisions. {\textcopyright} SISSA 2007.},
archivePrefix = {arXiv},
arxivId = {0709.2092},
author = {Frixione, Stefano and Nason, Paolo and Oleari, Carlo},
doi = {10.1088/1126-6708/2007/11/070},
eprint = {0709.2092},
file = {:home/harry/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamilton et al. - 2007 - Matching NLO QCD computations with parton shower simulations the POWHEG method.pdf:pdf},
issn = {11266708},
journal = {J. High Energy Phys.},
keywords = {Hadronic colliders,NLO computations,QCD},
month = {nov},
number = {11},
pages = {070},
publisher = {IOP Publishing},
title = {{Matching NLO QCD computations with parton shower simulations: The POWHEG method}},
url = {https://iopscience.iop.org/article/10.1088/1126-6708/2007/11/070 https://iopscience.iop.org/article/10.1088/1126-6708/2007/11/070/meta},
volume = {2007},
year = {2007}
}
@techreport{ATLAS-TDR-TDAQ-PhaseI,
abstract = {The Phase-I upgrade of the ATLAS Trigger and Data Acquisition (TDAQ) system is to allow the ATLAS experiment to efficiently trigger and record data at instantaneous luminosities that are up to three times that of the original LHC design while maintaining trigger thresholds close to those used in the initial run of the LHC.},
author = {{The ATLAS Collaboration}},
booktitle = {ATLAS-TDR-023-2013},
file = {:home/harry/Documents/phd/l1calo/resources/tdaq-phase1-tdr_2013.pdf:pdf},
title = {{Technical Design Report for the Phase-I Upgrade of the ATLAS TDAQ System}},
url = {http://cds.cern.ch/record/1602235/},
year = {2013}
}
@article{Efron1979,
abstract = {This is a survey article concerning recent advances in certain areas of statistical theory, written for a mathematical audience with no background in statistics. The topics are chosen to illustrate a special point: how the advent of the high-speed computer has affected the development of statistical theory. The topics discussed include nonparametric methods, the jackknife, the bootstrap, cross-validation, error-rate estimation in discriminant analysis, robust estimation, the influence function, censored data, the EM algorithm, and Cox's likelihood function. The exposition is mainly by example, with only a little offered in the way of theoretical development.},
author = {Efron, Bradley},
doi = {10.1137/1021092},
issn = {0036-1445},
journal = {SIAM Rev.},
month = {oct},
number = {4},
pages = {460--480},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Computers and the Theory of Statistics: Thinking the Unthinkable}},
url = {https://epubs.siam.org/doi/10.1137/1021092},
volume = {21},
year = {1979}
}
