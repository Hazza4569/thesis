Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Efron1979,
abstract = {This is a survey article concerning recent advances in certain areas of statistical theory, written for a mathematical audience with no background in statistics. The topics are chosen to illustrate a special point: how the advent of the high-speed computer has affected the development of statistical theory. The topics discussed include nonparametric methods, the jackknife, the bootstrap, cross-validation, error-rate estimation in discriminant analysis, robust estimation, the influence function, censored data, the EM algorithm, and Cox's likelihood function. The exposition is mainly by example, with only a little offered in the way of theoretical development.},
author = {Efron, Bradley},
doi = {10.1137/1021092},
issn = {0036-1445},
journal = {SIAM Rev.},
month = {oct},
number = {4},
pages = {460--480},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Computers and the Theory of Statistics: Thinking the Unthinkable}},
url = {https://epubs.siam.org/doi/10.1137/1021092},
volume = {21},
year = {1979}
}
@inproceedings{Deviveiros2019,
author = {Deviveiros, Pier-Olivier},
month = {aug},
publisher = {ATLAS Internal},
title = {{Run 3 L1 algorithm and trigger performance forum}},
url = {https://indico.cern.ch/event/839580/},
year = {2019}
}
@inproceedings{Brawn2019,
note = {[ATLAS Internal]},
author = {Brawn, Ian},
file = {:home/harry/Documents/phd/l1calo/resources/L1Calo{\_}191022{\_}Overview.pdf:pdf},
month = {oct},
title = {{L1Calo Overview, Status, Installation {\&} Commissioning}},
url = {https://indico.cern.ch/event/829769/contributions/3572289},
year = {2019}
}
@article{Efron1988,
abstract = {This is a survey of modern developments in statistical regression, written for the mathematically educated nonstatistician. It begins with a review of the traditional theory of least-squares curve-fitting. Modern developments in regression theory have developed in response to the practical limitations of the least-squares approach. Recent progress has been made feasible by the electronic computer, which frees statisticians from the confines of mathematical tractability. Topics discussed include robust regression, bootstrap measures of variability, local smoothing and cross-validation, projection pursuit, Mallows' Cp criterion, Stein estimation, generalized regression for Poisson data, and regression methods for censored data. All of the methods are illustrated with real-life examples.},
author = {Efron, Bradley},
doi = {10.1137/1030093},
issn = {00361445},
journal = {SIAM Rev.},
keywords = {62-02,62505,Poisson regression,Stein estimation,bootstrap,cross-validation Mallows' Cp,least absolute deviations,projection pursuit,robust regression},
month = {jul},
number = {3},
pages = {421--449},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Computer-intensive methods in statistical regression}},
url = {https://epubs.siam.org/doi/10.1137/1030093},
volume = {30},
year = {1988}
}
@article{Efron1987,
abstract = {We consider the problem of setting approximate confidence intervals for a single parameter $\theta$ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, $\theta$̂ ± $\sigma$̂z($\alpha$), can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.},
author = {Efron, Bradley},
doi = {10.2307/2289144},
issn = {01621459},
journal = {J. Am. Stat. Assoc.},
month = {mar},
number = {397},
pages = {171},
publisher = {JSTOR},
title = {{Better Bootstrap Confidence Intervals}},
volume = {82},
year = {1987}
}
@techreport{ATLAS-TDR-TDAQ-PhaseII,
address = {Geneva},
author = {{The ATLAS Collaboration}},
booktitle = {ATLAS-TDR-029},
file = {:home/harry/Documents/phd/l1calo/resources/tdaq-phase2-tdr{\_}2017.pdf:pdf},
institution = {CERN},
title = {{Technical Design Report for the Phase-II Upgrade of the ATLAS TDAQ System}},
url = {https://cds.cern.ch/record/2285584},
year = {2017}
}
@article{Hayes1989,
abstract = {The bootstrap statistical method is applied to the discrepancy in the one-charged-particle decay modes of the tau lepton. This eliminates questions about the correctness of the errors ascribed to the branching-fraction measurements and the use of Gaussian error distributions for systematic errors. The discrepancy is still seen when the results of the bootstrap analysis are combined with other measurements and with deductions from theory. But the bootstrap method assigns less statistical significance to the discrepancy compared to a method using Gaussian error distributions. {\textcopyright} 1989 The American Physical Society.},
author = {Hayes, Kenneth G. and Perl, Martin L. and Efron, Bradley},
doi = {10.1103/PhysRevD.39.274},
issn = {05562821},
journal = {Phys. Rev. D},
month = {jan},
number = {1},
pages = {274--279},
publisher = {American Physical Society},
title = {{Application of the bootstrap statistical method to the tau-decay-mode problem}},
url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.39.274},
volume = {39},
year = {1989}
}
@techreport{ATLAS-TDR-TDAQ-PhaseI,
abstract = {The Phase-I upgrade of the ATLAS Trigger and Data Acquisition (TDAQ) system is to allow the ATLAS experiment to efficiently trigger and record data at instantaneous luminosities that are up to three times that of the original LHC design while maintaining trigger thresholds close to those used in the initial run of the LHC.},
author = {{The ATLAS Collaboration}},
booktitle = {ATLAS-TDR-023-2013},
file = {:home/harry/Documents/phd/l1calo/resources/tdaq-phase1-tdr{\_}2013.pdf:pdf},
title = {{Technical Design Report for the Phase-I Upgrade of the ATLAS TDAQ System}},
url = {http://cds.cern.ch/record/1602235/},
year = {2013}
}
@article{ATLAS2020a,
abstract = {The ATLAS detector at the Large Hadron Collider reads out particle collision data from over 100 million electronic channels at a rate of approximately 100 kHz, with a recording rate for physics events of approximately 1 kHz. Before being certified for physics analysis at computer centres worldwide, the data must be scrutinised to ensure they are clean from any hardware or software related issues that may compromise their integrity. Prompt identification of these issues permits fast action to investigate, correct and potentially prevent future such problems that could render the data unusable. This is achieved through the monitoring of detector-level quantities and reconstructed collision event characteristics at key stages of the data processing chain. This paper presents the monitoring and assessment procedures in place at ATLAS during 2015-2018 data-taking. Through the continuous improvement of operational procedures, ATLAS achieved a high data quality efficiency, with 95.6$\backslash${\%} of the recorded proton-proton collision data collected at s=13 TeV certified for physics analysis.},
archivePrefix = {arXiv},
arxivId = {physics.ins-det/1911.04632},
author = {{The ATLAS Collaboration}},
doi = {10.1088/1748-0221/15/04/P04003},
eprint = {1911.04632},
issn = {17480221},
journal = {J. Instrum.},
keywords = {Large detector systems for particle and astroparti,Large detector-systems performance},
number = {4},
pages = {P04003},
primaryClass = {physics.ins-det},
title = {{ATLAS data quality operations and performance for 2015-2018 data-taking}},
volume = {15},
year = {2020}
}
@article{Aad2017a,
abstract = {The reconstruction of the signal from hadrons and jets emerging from the proton–proton collisions at the Large Hadron Collider (LHC) and entering the ATLAS calorimeters is based on a three-dimensional topological clustering of individual calorimeter cell signals. The cluster formation follows cell signal-significance patterns generated by electromagnetic and hadronic showers. In this, the clustering algorithm implicitly performs a topological noise suppression by removing cells with insignificant signals which are not in close proximity to cells with significant signals. The resulting topological cell clusters have shape and location information, which is exploited to apply a local energy calibration and corrections depending on the nature of the cluster. Topological cell clustering is established as a well-performing calorimeter signal definition for jet and missing transverse momentum reconstruction in ATLAS.},
author = {{The ATLAS Collaboration}},
doi = {10.1140/epjc/s10052-017-5004-5},
file = {:home/harry/Documents/phd/l1calo/resources/1603.02934.pdf:pdf},
issn = {1434-6052},
journal = {Eur. Phys. J. C},
number = {7},
pages = {490},
title = {{Topological cell clustering in the ATLAS calorimeters and its performance in LHC Run 1}},
url = {https://doi.org/10.1140/epjc/s10052-017-5004-5},
volume = {77},
year = {2017}
}
